{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "W0QSZgz_VniK",
        "0SIogySeVp8Z"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer minimalista para classificação de texto\n",
        "\n",
        "O objetivo é implementar e treinar um modelo transformer simples para classificação de texto no dataset SST2 (https://huggingface.co/datasets/stanfordnlp/sst2)\n",
        "\n",
        "Primeiramente, você deverá completar o código faltante no Transformer e depois testar melhorias para obter melhor desempenho."
      ],
      "metadata": {
        "id": "MAHasK0WUtuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalações e Imports"
      ],
      "metadata": {
        "id": "W0QSZgz_VniK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "aDK38vHWaCxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf229580-a027-4749-a393-19831759c62d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset   # facilita acesso ao dataset SST2\n",
        "import numpy as np\n",
        "from tqdm import tqdm     # barra de progresso\n"
      ],
      "metadata": {
        "id": "yNI71ZmMVkw0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer e carregamento do dataset"
      ],
      "metadata": {
        "id": "0SIogySeVp8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self, sentences, max_vocab_size=10000):\n",
        "        self.vocab = {}\n",
        "        self.inverse_vocab = {}\n",
        "        self.special_tokens = {\n",
        "            '[PAD]': 0,\n",
        "            '[UNK]': 1,\n",
        "            '[CLS]': 2,\n",
        "            '[SEP]': 3\n",
        "        }\n",
        "\n",
        "        # Build vocabulary from sentences\n",
        "        word_counts = Counter()\n",
        "        for sentence in sentences:\n",
        "            words = self._tokenize_text(sentence)\n",
        "            word_counts.update(words)\n",
        "\n",
        "        # Keep most frequent words\n",
        "        most_common = word_counts.most_common(max_vocab_size - len(self.special_tokens))\n",
        "\n",
        "        # Create vocabulary\n",
        "        for idx, (token, _) in enumerate(most_common, start=len(self.special_tokens)):\n",
        "            self.vocab[token] = idx\n",
        "            self.inverse_vocab[idx] = token\n",
        "\n",
        "        # Add special tokens to vocab\n",
        "        for token, idx in self.special_tokens.items():\n",
        "            self.vocab[token] = idx\n",
        "            self.inverse_vocab[idx] = token\n",
        "\n",
        "    def _tokenize_text(self, text):\n",
        "        # Simple whitespace tokenizer with lowercase and basic punctuation splitting\n",
        "        text = text.lower()\n",
        "        tokens = []\n",
        "        word = []\n",
        "        for char in text:\n",
        "            if char.isalnum():\n",
        "                word.append(char)\n",
        "            else:\n",
        "                if word:\n",
        "                    tokens.append(''.join(word))\n",
        "                    word = []\n",
        "                if char.strip():  # If it's not whitespace\n",
        "                    tokens.append(char)\n",
        "        if word:\n",
        "            tokens.append(''.join(word))\n",
        "        return tokens\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return self._tokenize_text(text)\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return [self.vocab.get(token, self.special_tokens['[UNK]']) for token in tokens]\n",
        "\n",
        "    def encode(self, text, max_length=None, padding=None, truncation=None, return_tensors=None):\n",
        "        tokens = ['[CLS]'] + self.tokenize(text)\n",
        "\n",
        "        if truncation and max_length:\n",
        "            tokens = tokens[:max_length]  # já leva em conta o [CLS]\n",
        "\n",
        "        input_ids = self.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # Handle padding\n",
        "        if padding == \"max_length\" and max_length:\n",
        "            if len(input_ids) < max_length:\n",
        "                input_ids = input_ids + [self.special_tokens['[PAD]']] * (max_length - len(input_ids))\n",
        "            elif len(input_ids) > max_length:\n",
        "                input_ids = input_ids[:max_length]\n",
        "\n",
        "        # Handle return_tensors\n",
        "        if return_tensors == \"pt\":\n",
        "            return {\"input_ids\": torch.tensor([input_ids])}\n",
        "        return {\"input_ids\": input_ids}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n"
      ],
      "metadata": {
        "id": "0Wk7q6NWV0GP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encapsulamento do Dataset"
      ],
      "metadata": {
        "id": "wi_DIvSuWIS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_length):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        encoding = self.tokenizer.encode(\n",
        "            text=item[\"sentence\"],\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),  # Remove batch dimension\n",
        "            \"label\": torch.tensor(item[\"label\"])\n",
        "        }"
      ],
      "metadata": {
        "id": "3kU3yeB-WGI0"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Carregar dataset"
      ],
      "metadata": {
        "id": "VDTvP6wYWc-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"sst2\")\n",
        "train_dataset = dataset[\"train\"].select(range(1000))\n",
        "val_dataset = dataset[\"validation\"].select(range(200))\n",
        "\n",
        "# Initialize tokenizer with raw sentences, to avoid circular dependence\n",
        "train_sentences = [item[\"sentence\"] for item in train_dataset]\n",
        "tokenizer = SimpleTokenizer(train_sentences, max_vocab_size=5000)\n",
        "max_length = 64\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TextClassificationDataset(train_dataset, tokenizer, max_length)\n",
        "val_dataset = TextClassificationDataset(val_dataset, tokenizer, max_length)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "0IUh1OaVWcWl"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Componentes do Transformer"
      ],
      "metadata": {
        "id": "KGglJidNW8Ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoding posicional"
      ],
      "metadata": {
        "id": "5ITsmaTUahrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_seq_length: int = 512):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_seq_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n"
      ],
      "metadata": {
        "id": "ehkzVstrW7XZ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cabeça de Atenção"
      ],
      "metadata": {
        "id": "gnuw-rEwasn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercicios:**\n",
        "\n",
        "1) Preencha o codigo no `__init__` para inicializar as projeções Wq, Wk e Wv. Elas serão projeções lineares (nn.Linear), equivalentes a uma camada de MLP (matriz de pesos e bias).\n",
        "\n",
        "No forward, elas vão receber a entrada (x) e gerar Q, K e V, respectivamente.\n",
        "\n",
        "O cuidado que vc precisa ter é com o formato das projeções. Elas vão receber uma entrada com dimensão n x d_model (onde n é o numero de tokens). Elas tem que devolver uma matriz n x d_model.\n",
        "\n",
        "Um exemplo, se a entrada tem 4 tokens e cada token é representado por um embedding com dimensão 5 (entrada é 4x5), a projeção Wq deve ser `nn.Linear(5,5)`.\n",
        "\n",
        "Já a escala (que vai dividir o Q x K transposto) é a raiz quadrada de d_model (conforme o artigo do Vaswani et al.)\n",
        "\n",
        "2) Complete o forward, implementando o cálculo da atenção. Observe que Q, K e V já são gerados usando as projeções que você definiu no `__init__`. Para facilitar o debugging, primeiro calcule:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Então aplique o softmax: ![image.png](data:image/png;base64,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)\n",
        "\n",
        "Por fim, calcule e retorne A = pesos x V, que é a saída do módulo de atenção.\n",
        "\n",
        "**==== Cuidados ====**\n",
        "\n",
        "Por questões de otimização do loop de treino, a entrada x não é apenas uma matriz (uma frase), mas sim um lote de frases (pense em uma lista de matrizes). Então a primeira dimensão de x é o número de frases (B -- batch size), a segunda é a quantidade de tokens (T) e a terceira é a dimensão do embedding de cada token (C -- alguém achou que C era uma letra legal pra representar isso e acabou pegando...).\n",
        "\n",
        "Então, Q, K e V terão o mesmo formato de X: B x T x C.\n",
        "\n",
        "Assim, para gerar K transposto corretamente, use `K.transpose(1,2)` para transpor as dimensões T (1) e C (2).\n",
        "\n",
        "Para implementar a multiplicação de matrizes Z = A x B, use `Z = A @ B` ou `Z = torch.bmm(A,B)`. Dará o mesmo resultado, mas a segunda é mais otimizada pois já supõe que a A e B tem 3 dimensões. Se os tensores A e B representam 'lotes' de matrizes, Z também representará. Cada item do 'lote' em Z terá o resultado de um item do lote em A multiplicado pelo mesmo item do lote em B.\n",
        "\n",
        "Além disso, para calcular o softmax corretamente em uma matriz chamada `S`, use `torch.softmax(S, dim=-1)`. Isso o aplicará na dimensão certa (cada linha de cada batch de S)."
      ],
      "metadata": {
        "id": "nnko32SLbHsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.Wq = nn.Linear(d_model, d_model)\n",
        "        self.Wk = nn.Linear(d_model, d_model)\n",
        "        self.Wv = nn.Linear(d_model, d_model)\n",
        "        self.scale = math.sqrt(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.Wq(x)\n",
        "        K = self.Wk(x)\n",
        "        V = self.Wv(x)\n",
        "\n",
        "        S = torch.bmm(Q, K.transpose(1,2)) / self.scale\n",
        "        pesos = torch.softmax(S, dim=-1)\n",
        "        A = torch.bmm(pesos, V)\n",
        "\n",
        "        return A\n",
        "\n"
      ],
      "metadata": {
        "id": "Wq_eohDIaqOS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Teste da inicialização do modulo de atenção"
      ],
      "metadata": {
        "id": "Z830myHfVD2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- formato das projeções (__init__) ---\n",
        "d_model = 4\n",
        "att = SingleHeadAttention(d_model)\n",
        "assert att.Wq.weight.shape == (4, 4)\n",
        "assert att.Wk.weight.shape == (4, 4)\n",
        "assert att.Wv.weight.shape == (4, 4)\n",
        "assert att.scale == 2\n",
        "print(\"✅ Formato das projeções e escala do modulo de atenção corretos.\")\n"
      ],
      "metadata": {
        "id": "uOwMo0qTU4T-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82312c3a-5fe9-48d3-a799-18c748428368"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Formato das projeções e escala do modulo de atenção corretos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Teste do cálculo de atenção\n",
        "\n",
        "Note que a entrada e a saída esperada são tensores 3d, com formato B x T x C = 1 x 2 x 2 (lote de 1 frase, com 2 tokens de dimensão 2)."
      ],
      "metadata": {
        "id": "uJiM5EUrhpaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 2\n",
        "att = SingleHeadAttention(d_model)\n",
        "\n",
        "# Configura projecoes customizadas pra dar o resultado pré-definido\n",
        "att.Wq.weight.data = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "att.Wq.bias.data.zero_()\n",
        "\n",
        "att.Wk.weight.data = torch.tensor([[0.5, -1.0], [1.5, 2.0]])\n",
        "att.Wk.bias.data.zero_()\n",
        "\n",
        "att.Wv.weight.data = torch.tensor([[2.0, 0.0], [0.0, 1.0]])\n",
        "att.Wv.bias.data.zero_()\n",
        "\n",
        "# Define input x: (batch_size B=1, seq_len T=2, d_model C=2)\n",
        "x = torch.tensor([[\n",
        "    [1.0, 0.0],   # token 1\n",
        "    [0.0, 1.0]    # token 2\n",
        "]])\n",
        "\n",
        "\n",
        "# Run model\n",
        "output = att(x)\n",
        "\n",
        "# previously-known expected output\n",
        "expected_output = torch.tensor([[\n",
        "    [1.000000, 0.500000], #contextual embedding of token 1\n",
        "    [1.339523, 0.330238]  #contextual embedding of token 2\n",
        "]])\n",
        "\n",
        "torch.set_printoptions(precision=6)\n",
        "\n",
        "# Compare\n",
        "assert torch.allclose(output, expected_output, atol=1e-5), f\"Expected {expected_output}, got {output}\"\n",
        "print(\"✅ Teste do calculo da atenção aprovado\")"
      ],
      "metadata": {
        "id": "hUBkqfIPWhke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "714b8d8c-247e-4c5b-a2db-cdb6c7fc3cf8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Teste do calculo da atenção aprovado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MLP"
      ],
      "metadata": {
        "id": "5oArYX2da2fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int = 2048, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout1(self.relu(self.linear1(x)))\n",
        "        x = self.dropout2(self.linear2(x))\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "EhX0qXuWa0mx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bloco Transformer\n",
        "\n",
        "Executa a atenção seguida do MLP. Há conexões residuais (saltos) da entrada pra saída da atenção e dela pra saída do MLP."
      ],
      "metadata": {
        "id": "RE_a4zVBa8OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int = 2048, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attention = SingleHeadAttention(d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attention_output = self.attention(x)\n",
        "        x = self.norm1(x + self.dropout(attention_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "qENkd4S9a6bK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer para classificação de texto\n",
        "\n",
        "O forward processa a entrada pra gerar a saída.\n",
        "\n",
        "A entrada (vetor de tokens) passa pelo token embedding (gerando a matriz (tokens x dimensões), encoding posicional, dropout e bloco transformer. Ao final, o embedding do token especial [CLS] é usado para classificação (sem softmax). O tokenizador anexa este token ao início de qualquer texto. Antes disso, usava-se um pooling (max ou mean) de cada embedding da sequência de tokens para gerar o embedding final que representasse toda a sequência.\n",
        "\n",
        "Este transformer não tem as conexões residuais e LayerNorm (é mais simples que o definido em Vaswani(2017)."
      ],
      "metadata": {
        "id": "FXsVIp5obin0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO testar mais blocos transformer (a saida de um bloco entra no seguinte)\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 256,  # dimensao dos embeddings\n",
        "        n_classes: int = 2,\n",
        "        max_seq_length: int = 64,\n",
        "        d_ff: int = 1024,    # num. de neuronios na camada nos MLP do transformer\n",
        "        dropout: float = 0.3,\n",
        "        num_blocks=2\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(d_model, d_ff, dropout) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        # classificador final (nao usa o d_ff, apenas o MLP do transformer usa)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model//2, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        # x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer_blocks(x)       # pode sequenciar mais de um\n",
        "\n",
        "        cls_output = x[:, 0, :]        # Extract [CLS] token embedding (position 0)\n",
        "        return self.classifier(cls_output)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sLHAlWnkbh0H"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "rdLqAqZzXk8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs, learning_rate, device):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # Added weight decay\n",
        "\n",
        "    best_val_acc = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss, train_correct, train_total = 0, 0, 0\n",
        "\n",
        "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} [Train]')\n",
        "        for batch in train_pbar:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Gradient clipping (estabiliza o treinamento)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            train_pbar.set_postfix({\n",
        "                'loss': f'{train_loss/(train_pbar.n+1):.4f}',\n",
        "                'acc': f'{100*train_correct/train_total:.2f}%'\n",
        "            })\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch + 1}/{num_epochs} [Val]')\n",
        "            for batch in val_pbar:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                labels = batch[\"label\"].to(device)\n",
        "\n",
        "                outputs = model(input_ids)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                val_pbar.set_postfix({\n",
        "                    'loss': f'{val_loss/(val_pbar.n+1):.4f}',\n",
        "                    'acc': f'{100*val_correct/val_total:.2f}%'\n",
        "                })\n",
        "\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f'New best model saved with val acc: {val_acc:.2f}%')\n",
        "\n",
        "        print(f'\\nEpoch {epoch + 1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {train_loss/len(train_loader):.4f} | Acc: {100*train_correct/train_total:.2f}%')\n",
        "        print(f'Val Loss: {val_loss/len(val_loader):.4f} | Acc: {val_acc:.2f}%')\n",
        "        print('-' * 60)"
      ],
      "metadata": {
        "id": "atSNUrGGXnsj"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instanciar e treinar modelo"
      ],
      "metadata": {
        "id": "LytxUTeRXV0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    vocab_size=len(tokenizer),\n",
        "    d_model=64,   # testar outros valores aqui\n",
        "    n_classes=2,\n",
        "    max_seq_length=max_length,\n",
        "    d_ff=64,      # testar outros valores aqui\n",
        "    dropout=0,     # testar outros valores aqui\n",
        "    num_blocks=2  # testar outros valores aqui\n",
        ").to(device)\n",
        "\n",
        "# Train the model\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    num_epochs=5,\n",
        "    learning_rate=0.0001,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "BRlPoE1qRdMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a753672-f8df-4779-dc68-1f507250174c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5 [Train]: 100%|██████████| 32/32 [00:02<00:00, 10.99it/s, loss=0.6925, acc=53.40%]\n",
            "Epoch 1/5 [Val]: 100%|██████████| 7/7 [00:00<00:00, 22.77it/s, loss=0.6917, acc=49.50%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best model saved with val acc: 49.50%\n",
            "\n",
            "Epoch 1/5:\n",
            "Train Loss: 0.6925 | Acc: 53.40%\n",
            "Val Loss: 0.6917 | Acc: 49.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5 [Train]: 100%|██████████| 32/32 [00:02<00:00, 12.08it/s, loss=0.6904, acc=54.20%]\n",
            "Epoch 2/5 [Val]: 100%|██████████| 7/7 [00:00<00:00, 28.98it/s, loss=1.2105, acc=49.50%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/5:\n",
            "Train Loss: 0.6904 | Acc: 54.20%\n",
            "Val Loss: 0.6917 | Acc: 49.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5 [Train]: 100%|██████████| 32/32 [00:04<00:00,  6.92it/s, loss=0.6899, acc=54.20%]\n",
            "Epoch 3/5 [Val]: 100%|██████████| 7/7 [00:00<00:00,  7.98it/s, loss=0.6914, acc=49.50%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/5:\n",
            "Train Loss: 0.6899 | Acc: 54.20%\n",
            "Val Loss: 0.6914 | Acc: 49.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5 [Train]: 100%|██████████| 32/32 [00:07<00:00,  4.32it/s, loss=0.6905, acc=54.20%]\n",
            "Epoch 4/5 [Val]: 100%|██████████| 7/7 [00:00<00:00, 20.74it/s, loss=0.6917, acc=49.50%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/5:\n",
            "Train Loss: 0.6905 | Acc: 54.20%\n",
            "Val Loss: 0.6917 | Acc: 49.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5 [Train]: 100%|██████████| 32/32 [00:01<00:00, 22.15it/s, loss=0.7124, acc=54.30%]\n",
            "Epoch 5/5 [Val]: 100%|██████████| 7/7 [00:00<00:00, 52.72it/s, loss=0.8072, acc=49.50%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/5:\n",
            "Train Loss: 0.6901 | Acc: 54.30%\n",
            "Val Loss: 0.6919 | Acc: 49.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inferência pós-treino"
      ],
      "metadata": {
        "id": "sgqBY2HgRW_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example inference\n",
        "def predict_sentiment(model, tokenizer, text, device):\n",
        "    model.eval()\n",
        "    encoding = tokenizer.encode(\n",
        "        text,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        predictions = torch.softmax(outputs, dim=1)\n",
        "        predicted_class = torch.argmax(predictions, dim=1)\n",
        "        confidence = predictions[0][predicted_class].item()\n",
        "\n",
        "    sentiment = \"Positive\" if predicted_class.item() == 1 else \"Negative\"\n",
        "    return sentiment, confidence\n",
        "\n",
        "# Test the model with a sample sentence\n",
        "sample_text = \"This movie was really enjoyable and well-made!\"\n",
        "sentiment, confidence = predict_sentiment(model, tokenizer, sample_text, device)\n",
        "print(f\"\\nText: {sample_text}\")\n",
        "print(f\"Predicted sentiment: {sentiment} (confidence: {confidence:.2f})\")"
      ],
      "metadata": {
        "id": "1N8WyvppYw2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aadc7d0a-d27b-4c03-b109-2bfcd3310476"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text: This movie was really enjoyable and well-made!\n",
            "Predicted sentiment: Positive (confidence: 0.54)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Melhorias\n",
        "\n",
        "Note que o modelo treina rápido, mas o desempenho é muito ruim: mesmo no treino, não consegue se distanciar de um palpite aleatório. Mude as configurações do modelo (dimensões do embedding (d_model), dimensões do MLP (d_ff), dropout, sequenciar mais blocos Transformer, etc.) e de treino (num. épocas, learning rate, etc.) para chegar num desempenho melhor.\n",
        "\n",
        "É possível chegar a uma acurácia de treino/teste de 80%/60%, mas não se estresse se não conseguir chegar nesses valores. Faça o seu melhor!"
      ],
      "metadata": {
        "id": "DAbmaKtfRcUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    vocab_size=len(tokenizer),\n",
        "    d_model=128,   # testar outros valores aqui\n",
        "    n_classes=2,\n",
        "    max_seq_length=max_length,\n",
        "    d_ff=256,      # testar outros valores aqui\n",
        "    dropout=0.1,     # testar outros valores aqui\n",
        "    num_blocks=4  # testar outros valores aqui\n",
        ").to(device)\n",
        "\n",
        "# Train the model\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    num_epochs=10,\n",
        "    learning_rate=0.0005,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9GdQW66euPB",
        "outputId": "030d1b0c-3017-4011-c029-e5f2f2f3b6be"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 32/32 [00:05<00:00,  5.54it/s, loss=0.6980, acc=51.30%]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 7/7 [00:00<00:00, 16.02it/s, loss=0.6915, acc=49.50%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best model saved with val acc: 49.50%\n",
            "\n",
            "Epoch 1/10:\n",
            "Train Loss: 0.6980 | Acc: 51.30%\n",
            "Val Loss: 0.6915 | Acc: 49.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 32/32 [00:06<00:00,  4.67it/s, loss=0.6916, acc=53.80%]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 7/7 [00:00<00:00, 15.84it/s, loss=0.6902, acc=49.50%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/10:\n",
            "Train Loss: 0.6916 | Acc: 53.80%\n",
            "Val Loss: 0.6902 | Acc: 49.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 32/32 [00:05<00:00,  5.64it/s, loss=0.6973, acc=52.40%]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 7/7 [00:00<00:00, 16.22it/s, loss=0.6932, acc=49.50%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/10:\n",
            "Train Loss: 0.6973 | Acc: 52.40%\n",
            "Val Loss: 0.6932 | Acc: 49.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 32/32 [00:06<00:00,  4.70it/s, loss=0.6842, acc=55.30%]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 7/7 [00:00<00:00, 16.51it/s, loss=0.6638, acc=59.00%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best model saved with val acc: 59.00%\n",
            "\n",
            "Epoch 4/10:\n",
            "Train Loss: 0.6842 | Acc: 55.30%\n",
            "Val Loss: 0.6638 | Acc: 59.00%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 [Train]: 100%|██████████| 32/32 [00:05<00:00,  5.56it/s, loss=0.6376, acc=63.00%]\n",
            "Epoch 5/10 [Val]: 100%|██████████| 7/7 [00:00<00:00, 13.36it/s, loss=0.6485, acc=62.50%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best model saved with val acc: 62.50%\n",
            "\n",
            "Epoch 5/10:\n",
            "Train Loss: 0.6376 | Acc: 63.00%\n",
            "Val Loss: 0.6485 | Acc: 62.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 [Train]: 100%|██████████| 32/32 [00:06<00:00,  4.76it/s, loss=0.5357, acc=74.00%]\n",
            "Epoch 6/10 [Val]: 100%|██████████| 7/7 [00:00<00:00, 16.50it/s, loss=0.6688, acc=65.00%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best model saved with val acc: 65.00%\n",
            "\n",
            "Epoch 6/10:\n",
            "Train Loss: 0.5357 | Acc: 74.00%\n",
            "Val Loss: 0.6688 | Acc: 65.00%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 [Train]: 100%|██████████| 32/32 [00:05<00:00,  5.43it/s, loss=0.4016, acc=83.10%]\n",
            "Epoch 7/10 [Val]: 100%|██████████| 7/7 [00:00<00:00, 10.63it/s, loss=0.7485, acc=61.00%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/10:\n",
            "Train Loss: 0.4016 | Acc: 83.10%\n",
            "Val Loss: 0.7485 | Acc: 61.00%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 [Train]: 100%|██████████| 32/32 [00:06<00:00,  4.90it/s, loss=0.2788, acc=89.20%]\n",
            "Epoch 8/10 [Val]: 100%|██████████| 7/7 [00:00<00:00, 16.66it/s, loss=0.9440, acc=60.50%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/10:\n",
            "Train Loss: 0.2788 | Acc: 89.20%\n",
            "Val Loss: 0.9440 | Acc: 60.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 [Train]: 100%|██████████| 32/32 [00:06<00:00,  5.24it/s, loss=0.1782, acc=93.00%]\n",
            "Epoch 9/10 [Val]: 100%|██████████| 7/7 [00:00<00:00, 10.52it/s, loss=1.2189, acc=61.50%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/10:\n",
            "Train Loss: 0.1782 | Acc: 93.00%\n",
            "Val Loss: 1.2189 | Acc: 61.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 [Train]: 100%|██████████| 32/32 [00:06<00:00,  5.14it/s, loss=0.1223, acc=95.80%]\n",
            "Epoch 10/10 [Val]: 100%|██████████| 7/7 [00:00<00:00, 15.48it/s, loss=1.5358, acc=57.00%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/10:\n",
            "Train Loss: 0.1223 | Acc: 95.80%\n",
            "Val Loss: 1.5358 | Acc: 57.00%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example inference\n",
        "def predict_sentiment(model, tokenizer, text, device):\n",
        "    model.eval()\n",
        "    encoding = tokenizer.encode(\n",
        "        text,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        predictions = torch.softmax(outputs, dim=1)\n",
        "        predicted_class = torch.argmax(predictions, dim=1)\n",
        "        confidence = predictions[0][predicted_class].item()\n",
        "\n",
        "    sentiment = \"Positive\" if predicted_class.item() == 1 else \"Negative\"\n",
        "    return sentiment, confidence\n",
        "\n",
        "# Test the model with a sample sentence\n",
        "sample_text = \"This movie was really enjoyable and well-made!\"\n",
        "sentiment, confidence = predict_sentiment(model, tokenizer, sample_text, device)\n",
        "print(f\"\\nText: {sample_text}\")\n",
        "print(f\"Predicted sentiment: {sentiment} (confidence: {confidence:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dnpCeKiffHc",
        "outputId": "271c8d40-ce7e-41db-d5ed-f07e0c2f13ae"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text: This movie was really enjoyable and well-made!\n",
            "Predicted sentiment: Positive (confidence: 0.98)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1BLZBjJGgXM0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}