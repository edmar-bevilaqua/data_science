{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Sales - Time Series Forecasting\n",
    "---\n",
    "\n",
    "Using machine learning to predict grocery sales\n",
    "\n",
    "In this notebook we will be solving the problem from the competiton: [Store Sales - Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting) from [Kaggle](https://www.kaggle.com/)\n",
    "\n",
    "### Summary:\n",
    "*   In this competition, you’ll use time-series forecasting to forecast store sales on data from **Corporación Favorita**, a large Ecuadorian-based grocery retailer.\n",
    "    *   Specifically, you'll build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores.  \n",
    "    You'll practice your machine learning skills with an approachable training dataset of dates, store, and item information, promotions, and unit sales.\n",
    "\n",
    "* The evaluation metric for this competition is Root Mean Squared Logarithmic Error - RMSLE.\n",
    "\n",
    "### File Descriptions and Data Field Information\n",
    "\n",
    "#### `train.csv`\n",
    "The training data, comprising time series of features store_nbr, family, and onpromotion as well as the target sales.\n",
    "\n",
    "*   **store_nbr** identifies the store at which the products are sold.\n",
    "*   **family** identifies the type of product sold.\n",
    "*   **sales** gives the total sales for a product family at a particular store at a given date.  \n",
    "    Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).\n",
    "*   **onpromotion** gives the total number of items in a product family that were being promoted at a store at a given date.\n",
    "\n",
    "#### `test.csv`\n",
    "The test data, having the same features as the training data.  \n",
    "You will predict the target sales for the dates in this file.  \n",
    "*   The dates in the test data are for the 15 days after the last date in the training data.\n",
    "\n",
    "#### `sample_submission.csv`\n",
    "*   A sample submission file in the correct format.\n",
    "\n",
    "#### `stores.csv`\n",
    "Store metadata, including city, state, type, and cluster.\n",
    "*   **cluster** is a grouping of similar stores.\n",
    "\n",
    "#### `oil.csv`\n",
    "Daily oil price.  \n",
    "Includes values during both the train and test data timeframes. (Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.)\n",
    "\n",
    "#### `holidays_events.csv`\n",
    "Holidays and Events, with metadata  \n",
    "\n",
    "\n",
    "### NOTES:\n",
    "Pay special attention to the transferred column.  \n",
    "*   A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government.\n",
    "\n",
    "*   A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is **Transfer**.  \n",
    "    *   For example, the holiday ***Independencia de Guayaquil*** was transferred from **2012-10-09** to **2012-10-12**, which means it was celebrated on **2012-10-12**.  \n",
    "\n",
    "*   Days that are type **Bridge** are extra days that are added to a holiday (e.g., to extend the break across a long weekend).  \n",
    "\n",
    "*   These are frequently made up by the type **Work Day** which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the **Bridge**.\n",
    "\n",
    "*   Additional holidays are days added a regular calendar holiday, for example, as typically happens around **Christmas** (making Christmas Eve a holiday).\n",
    "\n",
    "*   Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. **Supermarket sales could be affected by this.**\n",
    "\n",
    "*   A magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting file from the `.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_path = Path(\"data/train.csv\")\n",
    "if not file_path.is_file():\n",
    "    with zipfile.ZipFile(\"./store-sales-time-series-forecasting.zip\", 'r') as zf:\n",
    "        zf.extractall(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the `train.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_path = \"./data/train.csv\"\n",
    "\n",
    "sales = pd.read_csv(\n",
    "    train_path,\n",
    "    usecols=['store_nbr', 'family', 'date', 'sales'],\n",
    "    dtype={\n",
    "        'store_nbr': 'category',\n",
    "        'family': 'category',\n",
    "        'onpromotion': 'uint32',\n",
    "    },\n",
    "    parse_dates=['date']\n",
    "    )\n",
    "\n",
    "sales = sales.set_index('date').to_period('D')\n",
    "sales = sales.set_index(['store_nbr', 'family'], append=True)\n",
    "average_sales = sales.groupby('date').mean()['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the dataset is composed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = pd.read_csv(\"./data/stores.csv\")\n",
    "\n",
    "stores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining some parameters for the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "plt.rc(\n",
    "    \"figure\",\n",
    "    autolayout=True,\n",
    "    figsize=(11, 4),\n",
    "    titlesize=18,\n",
    "    titleweight='bold',\n",
    ")\n",
    "\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    "    legend=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a graph for the average sales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trend = average_sales.rolling(\n",
    "    window=365,\n",
    "    center=True,\n",
    "    min_periods=183,\n",
    ").mean()\n",
    "\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = average_sales.plot(**plot_params, alpha=0.5)\n",
    "ax = trend.plot(ax=ax, linewidth=3)\n",
    "ax.set_title(\"Average Sales (2013 - 2017)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our first model for TimeSeries\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the model:\n",
    "*   First let's unstack our index, making every index from the MultiIndex became a feature;\n",
    "*   For this model we will use DeterministicProcess from statsmodels\n",
    "*   We will be also using CalendarFourier from statsmodels tsa (Time Series Analysis) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_targets = sales.unstack(['store_nbr', 'family'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first model, let's start using only 1 year -> **2017**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_targets = y_targets.loc['2017']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
    "\n",
    "fourier = CalendarFourier(\n",
    "    freq='M',\n",
    "    order=4\n",
    ")\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "    index=y_targets.index,\n",
    "    constant=True,\n",
    "    order=1,\n",
    "    seasonal=True,\n",
    "    additional_terms=[fourier],\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "X = dp.in_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['NewYear'] = (X.index.dayofyear == 1)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, y_targets)\n",
    "\n",
    "y_predicted = pd.DataFrame(\n",
    "    data=model.predict(X),\n",
    "    index=X.index,\n",
    "    columns=y_targets.columns\n",
    ")\n",
    "\n",
    "y_predicted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how or models work for a specified family:\n",
    "*   In this case we will use `family = 'AUTOMOTIVE'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAMILY = 'AUTOMOTIVE'\n",
    "STORE = '1'\n",
    "\n",
    "X_forecasted = dp.out_of_sample(30)\n",
    "X_forecasted[\"NewYear\"] = (X_forecasted.index.dayofyear == 1)\n",
    "\n",
    "y_forecasted = pd.DataFrame(\n",
    "    data=model.predict(X_forecasted),\n",
    "    index=X_forecasted.index,\n",
    "    columns=y_targets.columns\n",
    ")\n",
    "\n",
    "ax = y_targets.loc(axis=1)['sales', STORE, FAMILY].plot(**plot_params, label='Sales')\n",
    "ax = y_predicted.loc(axis=1)['sales', STORE, FAMILY].plot(ax=ax, label='Predicted')\n",
    "ax = y_forecasted.loc(axis=1)['sales', STORE, FAMILY].plot(ax=ax, label='Forecasted')\n",
    "ax.set_title(f'{FAMILY} Sales at Store {STORE}')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the first submission to Kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "    './data/test.csv',\n",
    "    dtype={\n",
    "        'store_nbr': 'category',\n",
    "        'family': 'category',\n",
    "        'onpromotion': 'uint32',\n",
    "    },\n",
    "    parse_dates=['date'],\n",
    ")\n",
    "df_test['date'] = df_test.date.dt.to_period('D')\n",
    "df_test = df_test.set_index(['date', 'store_nbr', 'family',]).sort_index()\n",
    "\n",
    "# Create features for test set\n",
    "X_test = dp.out_of_sample(steps=df_test.index.get_level_values('date').unique().shape[0])\n",
    "X_test.index.name = 'date'\n",
    "X_test['NewYear'] = (X_test.index.dayofyear == 1)\n",
    "\n",
    "\n",
    "y_submit = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y_targets.columns)\n",
    "y_submit = y_submit.stack(['store_nbr', 'family'])\n",
    "y_submit = y_submit.join(df_test['id']).reindex(columns=['id', 'sales'])\n",
    "y_submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using time series as features\n",
    "---\n",
    "Some time series properties can only be modeled as serially dependent properties, that is, using as features past values of the target series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged Series and Lag Plots\n",
    "To investigate possible serial dependence (like cycles) in a time series, we need to create \"lagged\" copies of the series. Lagging a time series means to shift its values forward one or more time steps, or equivalently, to shift the times in its index backward one or more steps. In either case, the effect is that the observations in the lagged series will appear to have happened later in time.\n",
    "\n",
    "This shows the monthly unemployment rate in the US (`y`) together with its first and second lagged series (`y_lag_1` and `y_lag_2`, respectively). Notice how the values of the lagged series are shifted forward in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check if `average sales` has cyclic behavior:\n",
    "\n",
    "average_sales_2017 = average_sales.loc['2017']\n",
    "\n",
    "fourier = CalendarFourier(\n",
    "    freq='M',\n",
    "    order=4\n",
    ")\n",
    "dp = DeterministicProcess(\n",
    "    index=average_sales_2017.index,\n",
    "    constant=True,\n",
    "    order=1,\n",
    "    seasonal=True,\n",
    "    drop=False,\n",
    "    additional_terms=[fourier]\n",
    ")\n",
    "\n",
    "X = dp.in_sample()\n",
    "X['NewYear'] = (X.index.dayofyear == 1)\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, average_sales_2017)\n",
    "\n",
    "average_deseason = average_sales_2017 - model.predict(X)\n",
    "average_deseason.name = 'average_sales_deseasoned'\n",
    "\n",
    "# Moving average\n",
    "moving_avg = average_deseason.rolling(\n",
    "    window=7,\n",
    "    center=True\n",
    ").mean()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "\n",
    "fig.set_size_inches(12, 6)\n",
    "\n",
    "average_deseason.plot(ax=ax1, title='Average Sales 2017 Deseasoned')\n",
    "moving_avg.plot(ax=ax2, title='Moving Average')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the average sales deseasoned and her moving average, we can't see clearly a cyclic behavior.\n",
    "\n",
    "Let's check for a specific Product Family: **SCHOOL AND OFFICE SUPPLIES** and see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(\n",
    "    train_path,\n",
    "    usecols=['family', 'date', 'sales', 'onpromotion'],\n",
    "    dtype={\n",
    "        'family': 'category',\n",
    "        'onpromotion': 'uint32',\n",
    "    },\n",
    "    parse_dates=['date']\n",
    "    )\n",
    "\n",
    "family_sales = (\n",
    "    sales\n",
    "    .groupby(['family', 'date'], observed=True)\n",
    "    .mean() \n",
    "    .unstack('family')\n",
    "    .loc['2017', ['sales', 'onpromotion']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ofc_sup_sales_2017 = family_sales.loc(axis=1)[:, 'SCHOOL AND OFFICE SUPPLIES']\n",
    "\n",
    "y = ofc_sup_sales_2017.loc[:, 'sales'].squeeze()\n",
    "\n",
    "fourier = CalendarFourier(\n",
    "    freq='M',\n",
    "    order=4\n",
    ")\n",
    "dp = DeterministicProcess(\n",
    "    index=average_sales_2017.index,\n",
    "    constant=True,\n",
    "    order=1,\n",
    "    seasonal=True,\n",
    "    drop=False,\n",
    "    additional_terms=[fourier]\n",
    ")\n",
    "\n",
    "fourier = CalendarFourier(freq='M', order=4)\n",
    "dp = DeterministicProcess(\n",
    "    constant=True,\n",
    "    index=y.index,\n",
    "    order=1,\n",
    "    seasonal=True,\n",
    "    drop=True,\n",
    "    additional_terms=[fourier],\n",
    ")\n",
    "X_time = dp.in_sample()\n",
    "X_time['NewYearsDay'] = (X_time.index.dayofyear == 1)\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X_time, y)\n",
    "y_deseason = y - model.predict(X_time)\n",
    "y_deseason.name = 'sales_deseasoned'\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "fig.set_size_inches(12, 6)\n",
    "\n",
    "y_deseason.plot(ax=ax1)\n",
    "ax1.set_title(\"Sales of School and Office Supplies (deseasonalized)\")\n",
    "\n",
    "y_ma = y.rolling(\n",
    "    window=7,\n",
    "    center=True\n",
    ").mean()\n",
    "\n",
    "y_ma.plot(ax=ax2)\n",
    "ax2.set_title(\"Seven-Day Moving Average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from utils import make_lags, make_leads\n",
    "\n",
    "onpromotion = ofc_sup_sales_2017.loc[:, 'onpromotion'].squeeze().rename('onpromotion')\n",
    "\n",
    "X_lags = make_lags(y_deseason, lags=1)\n",
    "\n",
    "X_promo = pd.concat([\n",
    "    make_lags(onpromotion, lags=1),\n",
    "    onpromotion,\n",
    "    make_leads(onpromotion, leads=1)\n",
    "], axis=1)\n",
    "\n",
    "X = pd.concat([X_lags, X_promo], axis=1)\n",
    "y, X = y.align(X, join='inner')\n",
    "\n",
    "X = X.fillna(0.0)\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=30, shuffle=False)\n",
    "\n",
    "model = LinearRegression(fit_intercept=False).fit(X_train, y_train)\n",
    "y_fit = pd.Series(model.predict(X_train), index=X_train.index).clip(0.0)\n",
    "y_pred = pd.Series(model.predict(X_valid), index=X_valid.index).clip(0.0)\n",
    "\n",
    "rmsle_train = mean_squared_log_error(y_train, y_fit)**0.5\n",
    "rmsle_valid = mean_squared_log_error(y_valid, y_pred)**0.5\n",
    "print(f'Training RMSLE: {rmsle_train:.5f}')\n",
    "print(f'Validation RMSLE: {rmsle_valid:.5f}')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = y.plot(**plot_params, alpha=0.5, title=\"Onpromotion Average Sales\", ylabel=\"items sold\")\n",
    "ax = y_fit.plot(ax=ax, label=\"Fitted\", color='C0')\n",
    "ax = y_pred.plot(ax=ax, label=\"Forecast\", color='C3')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second approach: Using all data to build a hybrid model\n",
    "---\n",
    "In this second attempt to decrease RMSLE, we will be using all of the Data that were provided for the competition:\n",
    "*   `train.csv`\n",
    "*   `stores.csv`\n",
    "*   `oil.csv`\n",
    "*   `holidays_events.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all libraries we are going to use:\n",
    "import sys\n",
    "import pathlib\n",
    "libpath = str(pathlib.Path().resolve().parent)\n",
    "sys.path.append(libpath)\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign all files to a variable:\n",
    "sales = pd.read_csv('./data/train.csv', dtype={'date':'str'}, parse_dates=['date'])\n",
    "stores = pd.read_csv('./data/stores.csv')\n",
    "oil = pd.read_csv('./data/oil.csv', dtype={'date':'str'}, parse_dates=['date']).rename(columns={'dcoilwtico':'oil_price'})\n",
    "holidays = pd.read_csv('./data/holidays_events.csv', dtype={'date':'str'}, parse_dates=['date'])\n",
    "\n",
    "train = sales.copy()\n",
    "test = pd.read_csv('./data/test.csv', dtype={'date':'str'}, parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oil -> train, test\n",
    "train = pd.merge(train, oil, on='date', how='left')\n",
    "test = pd.merge(test, oil, on='date', how='left')\n",
    "\n",
    "# stores -> train, test\n",
    "train = pd.merge(train, stores, on='store_nbr', how='left')\n",
    "test = pd.merge(test, stores, on='store_nbr', how='left')\n",
    "\n",
    "# Making holidays useful\n",
    "useless_days = (holidays['transferred'] == True) | (holidays['type'] == 'Work Day')\n",
    "tHolidays = holidays.drop(holidays[useless_days].index)\n",
    "tHolidays = tHolidays.drop(['type', 'description', 'transferred'], axis=1)\n",
    "tHolidays['holiday'] = 1\n",
    "tHolidays = tHolidays.drop(tHolidays[tHolidays['date'].duplicated()].index)\n",
    "\n",
    "# Splitting holidays by 'locale' -> Local, Regional, National\n",
    "local_holidays = tHolidays[tHolidays['locale'] == 'Local']\n",
    "regional_holidays = tHolidays[tHolidays['locale'] == 'Regional']\n",
    "national_holidays = tHolidays[tHolidays['locale'] == 'National'].drop(['locale', 'locale_name'], axis=1) \n",
    "\n",
    "# national_holidays -> train, test\n",
    "train = pd.merge(train, national_holidays, on='date', how='left')\n",
    "test = pd.merge(test, national_holidays, on='date', how='left')\n",
    "\n",
    "# regional_holidays -> train, test\n",
    "for date, state in zip(regional_holidays['date'], regional_holidays['locale_name']):\n",
    "    train.loc[(train['date'] == date) & (train['state'] == state), 'holiday'] = 1\n",
    "\n",
    "for date, state in zip(regional_holidays['date'], regional_holidays['locale_name']):\n",
    "    test.loc[(test['date'] == date) & (test['state'] == state), 'holiday'] = 1\n",
    "\n",
    "# local_holidays -> train, test\n",
    "for date, city in zip(local_holidays['date'], local_holidays['locale_name']):\n",
    "    train.loc[(train['date'] == date) & (train['city'] == city), 'holiday'] = 1\n",
    "\n",
    "for date, city in zip(local_holidays['date'], local_holidays['locale_name']):\n",
    "    test.loc[(test['date'] == date) & (test['city'] == city), 'holiday'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null values in the train set:\")\n",
    "display(train.isnull().sum())\n",
    "\n",
    "print(\"Null values in the test set:\")\n",
    "display(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Using the Dataframe.bfill and Dataframe.ffill methods:\n",
    "\n",
    "    *   `Dataframe.bfill()` method is used to backward fill the missing values in the dataset.  \n",
    "    It will backward fill the `NaN` values that are present in the pandas dataframe.\n",
    "\n",
    "    *   `Dataframe.ffill()` method is used to forward fill the missing values in the dataset.  \n",
    "    It will forward fill the `NaN` values that are present in the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train -> bfill() and ffill()\n",
    "train.loc[:, 'oil_price'] = train['oil_price'].bfill().ffill()\n",
    "\n",
    "# Test -> bfill() and ffill()\n",
    "test.loc[:, 'oil_price'] = test['oil_price'].bfill().ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null values in the train set:\")\n",
    "display(train.isnull().sum())\n",
    "\n",
    "print(\"Null values in the test set:\")\n",
    "display(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   For holidays, if the value is `NaN`, we will fill it with `0`, just to avoid problems on our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train -> fillna()\n",
    "train.loc[:, 'holiday'] = train['holiday'].fillna(value=0)\n",
    "\n",
    "# Test -> fillna()\n",
    "test.loc[:, 'holiday'] = test['holiday'].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null values in the train set:\")\n",
    "display(train.isnull().sum())\n",
    "\n",
    "print(\"Null values in the test set:\")\n",
    "display(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   The column called `\"id\"` has the same values of our index, so we can either remove this feature or set it as our new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train -> set the 'id' feature as index\n",
    "train.set_index('id', inplace=True)\n",
    "\n",
    "# test -> set the 'id' feature as index\n",
    "test.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag Features\n",
    "\n",
    "Lag features are very commom in Time Series problems\n",
    "\n",
    "*   In this project we will be using 3 lag features:\n",
    "   *   7 days;\n",
    "   *   14 days;\n",
    "   *   28 days;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_sales(df, date_column, sales_column, lag_days=1):\n",
    "    df_copy = df.copy()\n",
    "    df_copy[date_column] += pd.Timedelta(days=lag_days)\n",
    "    df_copy.rename(columns={sales_column : f'sales_{lag_days}_days_ago'}, inplace=True)\n",
    "    df_copy = pd.merge(left=df,\n",
    "                       right=df_copy.loc[:, ['store_nbr', 'family', 'date', f'sales_{lag_days}_days_ago']],\n",
    "                       on=['store_nbr', 'family', 'date'],\n",
    "                       how='left')\n",
    "    df_copy.loc[:, f'sales_{lag_days}_days_ago'].fillna(0, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.concat([train, test])\n",
    "\n",
    "# Train -> Adding lag features for 7, 14 and 28 days:\n",
    "for days in [7, 14, 28]:\n",
    "    train = lag_sales(train, 'date', 'sales', lag_days=days)\n",
    "\n",
    "\n",
    "# creating lag features for the test set, using the last data from de train set\n",
    "for days in [7, 14, 28]:\n",
    "    temp_df = lag_sales(temp_df, 'date', 'sales', lag_days=days)\n",
    "\n",
    "test = temp_df.loc[temp_df['date'] >= test['date'].min(), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering:\n",
    "\n",
    "*   Let's create some features that will help us explain some behaviors of our data\n",
    "    *   For that we will be using a custom method called `add_datepart()` from a custom module I created called `utils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train -> Feature Engineering\n",
    "train = utils.add_datepart(df=train, fldnames='date', drop=False)\n",
    "\n",
    "# Test -> Feature Engineering\n",
    "test = utils.add_datepart(df=test, fldnames='date', drop=False)\n",
    "\n",
    "# This methods executes inplace, so theres no need to do \"train/test = utils.add_datepart(...)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Sales Feature\n",
    "\n",
    "Let's create a feature that computes the average sales, considering:\n",
    "*   Store: For that we will use `store_nbr`\n",
    "*   Family: The family of products -> `family`\n",
    "*   Day of the week: From the feature `Dayofweek`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = train.groupby(['store_nbr', 'family', 'Dayofweek'])['sales'].mean().reset_index()\n",
    "grouped_df.rename(columns={'sales' : 'avg_sales'}, inplace=True)\n",
    "train = train.merge(right= grouped_df,\n",
    "                    on=['store_nbr', 'family', 'Dayofweek'],\n",
    "                    how='left')\n",
    "test = test.merge(right= grouped_df,\n",
    "                    on=['store_nbr', 'family', 'Dayofweek'],\n",
    "                    how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Now, we should remove useless columns that won't add useful information to our model, such as:\n",
    "    *   `city`: We used the city and state feature to create our `'holiday'` feature, so we no longer need these feature.\n",
    "    *   `state`: Same as above.\n",
    "    *   `cluster`: This feature is about which store cluster the respective store belongs to, which is almost useless to our model.\n",
    "    *   `store_nbr`: Similar to the above feature, it only tells us the identifier to a certain store, which is almost useless to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train -> drop useless features\n",
    "#train = train.drop(['city', 'state', 'cluster', 'store_nbr'], axis=1)\n",
    "\n",
    "# Test -> drop useless features\n",
    "#test = test.drop(['city', 'state', 'cluster', 'store_nbr'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model:\n",
    "---\n",
    "We will use a validation set, consisting of:\n",
    "*   The last 15 days of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Define the column transformer\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse_output=False), train.select_dtypes('object').columns.to_list())\n",
    "    ],\n",
    "    remainder='passthrough'  # Keeps the other columns (not 'object') as is\n",
    ")\n",
    "\n",
    "transformed_df_train = column_transformer.fit_transform(train.select_dtypes('object'))\n",
    "transformed_df_test = column_transformer.transform(test.select_dtypes('object'))\n",
    "\n",
    "new_column_names = column_transformer.get_feature_names_out().tolist()\n",
    "\n",
    "# Convert the result back to a DataFrame\n",
    "transformed_df_train = pd.DataFrame(transformed_df_train.astype(np.int64), columns=new_column_names)\n",
    "transformed_df_test = pd.DataFrame(transformed_df_test.astype(np.int64), columns=new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train.drop(columns=train.select_dtypes('object').columns.tolist()), transformed_df_train], axis=1)\n",
    "test = pd.concat([test.drop(columns=test.select_dtypes('object').columns.tolist()).reset_index(drop=True), transformed_df_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.head())\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = train.loc[train['date'] >= '2017-08-01', :].copy()\n",
    "train = train.loc[train['date'] < '2017-08-01', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['date', 'sales'], axis=1)\n",
    "y_train = train.loc[:, 'sales']\n",
    "\n",
    "X_val = val_set.drop(['date', 'sales'], axis=1)\n",
    "y_val = val_set.loc[:, 'sales']\n",
    "\n",
    "X_test = test.drop(['date', 'sales'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA LEAKAGE ALERT:\n",
    "\n",
    "*   As we decided to use a validation set, we need to consider it as something that wasn't seen before, this implies that:\n",
    "    *   In our test set, the lag features for 7 and 14 days should be `NaN`, since those days belongs to the validation set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[:, ['sales_7_days_ago', 'sales_14_days_ago',]] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HistGradientBoostingRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "hgbr = HistGradientBoostingRegressor(random_state=100)\n",
    "\n",
    "hgbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = hgbr.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "msle = mean_squared_log_error(y_val, y_val_pred)\n",
    "rmsle = np.sqrt(msle)\n",
    "\n",
    "print(f'Root Mean Squared Log Error: {rmsle}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Second Submission to the Competition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "    './data/test.csv',\n",
    "    dtype={\n",
    "        'store_nbr': 'category',\n",
    "        'family': 'category',\n",
    "        'onpromotion': 'uint32',\n",
    "    },\n",
    "    parse_dates=['date'],\n",
    ")\n",
    "\n",
    "y_submit = pd.DataFrame(hgbr.predict(X_test), index=df_test.id, columns=['sales']).reset_index()\n",
    "y_submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Features:\n",
    "*   Let's add Fourier Features and select the best `order` hiperparameter using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all libraries we are going to use:\n",
    "import sys\n",
    "import pathlib\n",
    "libpath = str(pathlib.Path().resolve().parent)\n",
    "sys.path.append(libpath)\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign all files to a variable:\n",
    "sales = pd.read_csv('./data/train.csv', dtype={'date':'str'}, parse_dates=['date'])\n",
    "sales['date'] = pd.to_datetime(sales['date'])\n",
    "\n",
    "stores = pd.read_csv('./data/stores.csv')\n",
    "oil = pd.read_csv('./data/oil.csv', dtype={'date':'str'}, parse_dates=['date']).rename(columns={'dcoilwtico':'oil_price'})\n",
    "holidays = pd.read_csv('./data/holidays_events.csv', dtype={'date':'str'}, parse_dates=['date'])\n",
    "\n",
    "train = sales.copy()\n",
    "test = pd.read_csv('./data/test.csv', dtype={'date':'str'}, parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "sales_fam = sales.set_index('date')\n",
    "sales_fam = sales_fam.loc[(sales_fam['family'] == 'AUTOMOTIVE') & (sales_fam['store_nbr'] == 2), 'sales']\n",
    "\n",
    "fig = px.line(sales_fam, title=\"Sales from 2013 to 2017\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import CalendarFourier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def create_fourier_features(df, order):\n",
    "    fourier = CalendarFourier(freq='A', order=order)\n",
    "    dp = DeterministicProcess(\n",
    "        index=df.index,\n",
    "        constant=True,                # Add a constant term for the intercept\n",
    "        order=1,                      # Linear trend\n",
    "        seasonal=False,               # No seasonal terms\n",
    "        additional_terms=[fourier],   # Add Fourier terms\n",
    "        drop=True                     # Drop missing dates\n",
    "    )\n",
    "    return dp.in_sample()\n",
    "\n",
    "\n",
    "def cross_val_score_for_order(df, y, order):\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    mse_scores = []\n",
    "    \n",
    "    for train_index, test_index in tscv.split(df):\n",
    "        df_train, df_test = df.iloc[train_index], df.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Create Fourier features\n",
    "        X_train = create_fourier_features(df_train, order)\n",
    "        X_test = create_fourier_features(df_test, order)\n",
    "        \n",
    "        # Fit a model (e.g., linear regression)\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and calculate MSE\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_scores.append(mse)\n",
    "        \n",
    "    return np.mean(mse_scores)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate performance for different orders\n",
    "orders = range(1, 21)  # Define a range of orders to test\n",
    "results = {}\n",
    "\n",
    "for order in orders:\n",
    "    mse = cross_val_score_for_order(df, y, order)\n",
    "    results[order] = mse\n",
    "    print(f'Order: {order}, MSE: {mse}')\n",
    "\n",
    "# Find the best order with the lowest MSE\n",
    "best_order = min(results, key=results.get)\n",
    "print(f'Best order: {best_order}, with MSE: {results[best_order]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
